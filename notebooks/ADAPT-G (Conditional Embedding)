{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ADAPT-G: Adversarial Diffusion And Probabilistic Transformer-Graph for Financial Fraud Detection**\n\n---\n\n## Overview\n\n**ADAPT-G** is a novel hybrid architecture designed for **financial transaction and mobile money fraud detection**.  \nIt combines **adversarial curriculum learning with a diffusion-based generator** to create realistic synthetic fraud scenarios and a **probabilistic Transformer-Graph Neural Network (GNN) detector** to identify both **behavioral** (temporal) and **structural** (relational) anomalies.\n\nOur goal is to **push the boundaries of fraud detection** by addressing the weaknesses of traditional models in catching **stealthy, low-signal fraud patterns** — particularly those that exploit the **network relationships** between entities and **temporal transaction patterns**.\n\n---\n\n## Research Goals\n\n- **Develop** a hybrid adversarial architecture capable of detecting complex fraud signals in transaction networks.\n- **Incorporate** probabilistic reasoning into the detection stage for better uncertainty estimation and high-risk flagging.\n- **Benchmark** against state-of-the-art fraud detection models on real-world financial datasets.\n- **Evaluate** across both temporal and graph-based perspectives to capture full fraud signatures.\n\n---\n\n## Model Architecture\n\nThe **ADAPT-G** framework has two major components:\n\n1. **Diffusion-based Generator**  \n   - Generates high-quality, diverse fraudulent transaction sequences and relational patterns.  \n   - Uses **adversarial curriculum learning** to gradually create harder-to-detect fraudulent scenarios.\n\n2. **Probabilistic Transformer-GNN Detector**  \n   - **Transformer**: Models per-entity temporal transaction behavior.\n   - **Lightweight GNN**: Captures graph-level relational dependencies between entities (accounts, devices, merchants).  \n   - **Probabilistic Output Layer**: Produces calibrated uncertainty scores alongside classification outputs.\n\n---\n\n## Benchmark Models\n\nWe will compare ADAPT-G against:\n\n- **XGBoost** (tabular, feature-engineered baseline)\n- **GCN** (Graph Convolutional Network)\n- **GAT** (Graph Attention Network)\n- **Temporal GNNs** (TGAT, TGN)\n- **LSTM/GRU Autoencoders** (temporal anomaly detection baselines)\n- **TabTransformer** (tabular deep learning baseline)\n\n---\n\n## Datasets\n\nWe will evaluate on **multiple benchmark datasets** for financial/mobile money fraud detection:\n\n- **IEEE-CIS Fraud Detection** (Kaggle)\n- **PaySim** (Mobile money fraud simulation)\n- **Elliptic Dataset** (Bitcoin transaction fraud/illicit activity)\n- **SIMF Dataset** (Mobile money & transaction fraud – simulated and real-world mix)\n\nEach dataset will be preprocessed into both **tabular** and **graph-structured formats** where applicable, ensuring fair comparison across all models.\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Imports & Utilities**\n\n---\n\nThis section loads all **core libraries** required for the ADAPT-G framework and defines key **utility functions** for:\n\n1. **Seeding for reproducibility** — Ensures consistent experimental results.\n2. **Metric calculations** — Precision, Recall, F1-score, AUC-ROC, and more.\n3. **Visualization** — For training/evaluation curves, ROC plots, and confusion matrices.\n","metadata":{}},{"cell_type":"code","source":"# # Install required packages\n# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118  # adjust cu version or use cpu wheel\n!pip install torch-geometric torch-scatter torch-sparse torch-spline-conv torch-cluster --quiet  # follow torch_geometric install instructions for your torch version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T20:26:41.990161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Imports\n# ================================\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix,\n    accuracy_score\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchaudio\nimport torch_geometric\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, GATConv, global_mean_pool\nfrom datetime import datetime\n\n\nimport math\nfrom typing import List, Optional, Tuple\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Helper Functions\n# ================================\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Sets seed for reproducibility across Python, NumPy, and PyTorch.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"[INFO] Seed set to: {seed}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Metrics\n# -------------------------------------------------\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    \"\"\"\n    Calculate core classification metrics.\n    y_true: Ground truth labels\n    y_pred: Predicted labels\n    y_prob: Predicted probabilities (for ROC/AUC)\n    \"\"\"\n    metrics_dict = {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n    }\n    if y_prob is not None:\n        metrics_dict[\"AUC-ROC\"] = roc_auc_score(y_true, y_prob)\n    return metrics_dict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Visualization Utilities\n# -------------------------------------------------\ndef plot_loss_curves(train_losses, val_losses):\n    plt.figure(figsize=(8, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training & Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_roc_curve(y_true, y_prob):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    auc_score = roc_auc_score(y_true, y_prob)\n    plt.figure(figsize=(6, 6))\n    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, labels=None):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Model Architecture**\n\nThis section defines the two core components of **ADAPT-G**:\n\n---\n\n### **3.1 Generator — TabDiffusion Sample Simulator**  \nWe use **TabDiffusion** to generate **both fraudulent and non-fraudulent transaction samples**.  \n\n- **Conditional Generation:** Generator is conditioned on a label (`fraud = 0` or `1`) along with context features (merchant, device, account profile).  \n- **Dual Simulation:**  \n  - Fraudulent samples: Rare anomalies, generated to mimic realistic fraud signatures.  \n  - Non-fraudulent samples: Plausible “normal” transactions to maintain balance and realism.  \n- **Adversarial Curriculum:** Generator evolves to create increasingly **hard-to-discriminate samples**, forcing the detector to improve.  \n\n> Unlike standard anomaly generation approaches, TabDiffusion provides **balanced synthetic data augmentation** for both classes, addressing fraud rarity while avoiding biased training.  \n\n---\n\n### **3.2 Detector — Probabilistic Transformer-GNN**  \nThe detector integrates **graph structure** and **temporal behavior** to validate whether a given transaction is fraudulent or not:  \n\n- **Graph Encoder (GNN):** Captures structural dependencies across accounts, devices, and merchants.  \n- **Transformer Encoder:** Models sequential patterns in entity transaction histories.  \n- **Fusion Layer:** Combines graph + temporal embeddings.  \n- **Probabilistic Output Layer:**  \n  - Produces fraud classification (fraud vs. non-fraud).  \n  - Estimates prediction uncertainty via **MC Dropout**.  \n\n---\n\n### **3.3 Joint Training Paradigm**  \n- Generator (TabDiffusion) produces both **fraudulent and non-fraudulent samples**.  \n- Detector (Transformer-GNN) learns to confirm whether transactions are truly fraudulent or not.  \n- Adversarial training loop ensures:  \n  - Generator creates challenging, realistic samples.  \n  - Detector improves robustness by distinguishing real vs synthetic and fraud vs non-fraud simultaneously.  ","metadata":{}},{"cell_type":"markdown","source":"### **4.1 Generator: Tabular Diffusion**","metadata":{}},{"cell_type":"code","source":"# TabDiffusion-style mixed-type conditional generator (Transformer denoiser + mixed-type sampler)\n# Notebook-ready, experiment-ready implementation.\n# Author: (adapted for ADAPT-Fraud)\n# Requirements: torch >=1.12, (GPU highly recommended)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This class expects `cat_emb_modules` (ModuleDict) to exist in the notebook scope and will use them.\nclass TabDiffusionGeneratorRefactor(nn.Module):\n    def __init__(self, num_numeric, cat_cols_list, cat_emb_modules, cond_columns,\n                 token_dim=192, time_embed_dim=64, transformer_layers=4, transformer_heads=4, transformer_ff=512,\n                 uncond_prob=0.1):\n        super().__init__()\n        self.num_numeric = num_numeric\n        self.cat_cols_list = cat_cols_list\n        self.num_cat = len(cat_cols_list)\n        self.cat_emb_modules = cat_emb_modules  # expects keys matching cat_cols_list\n        self.cond_columns = cond_columns\n        self.token_dim = token_dim\n        self.time_embed_dim = time_embed_dim\n        self.uncond_prob = uncond_prob\n\n        # numeric projection to token_dim (per numeric feature -> token vector)\n        self.num_proj = nn.ModuleList([nn.Linear(1, token_dim) for _ in range(num_numeric)]) if num_numeric>0 else None\n\n        # time and cond projections\n        self.time_proj = nn.Sequential(nn.Linear(time_embed_dim, token_dim//2), nn.GELU(), nn.Linear(token_dim//2, token_dim//2))\n        # cond projection will be produced by concatenating embeddings of cond_columns + fraud bit -> then project\n        cond_input_dim = len(self.cond_columns) * CAT_EMBED_DIM + 1 if len(self.cond_columns)>0 else 1\n        self.cond_proj = nn.Sequential(nn.Linear(cond_input_dim, token_dim//2), nn.GELU(), nn.Linear(token_dim//2, token_dim//2))\n        self.cond_ln = nn.LayerNorm(token_dim)\n\n        # Transformer denoiser\n        layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=transformer_heads, dim_feedforward=transformer_ff, batch_first=True)\n        self.transformer = nn.TransformerEncoder(layer, num_layers=transformer_layers)\n        self.ln = nn.LayerNorm(token_dim)\n\n        # per-token logvar\n        total_feat_tokens = num_numeric + self.num_cat\n        self.log_var_tokens = nn.Parameter(torch.zeros(total_feat_tokens, token_dim))\n\n        # noise head and decoders\n        self.noise_head = nn.Linear(token_dim, token_dim)\n        self.cat_decoders = nn.ModuleList([nn.Linear(token_dim, int(cat_emb_modules[c].num_embeddings)) for c in cat_cols_list])\n        self.num_readout = nn.ModuleList([nn.Linear(token_dim,1) for _ in range(num_numeric)]) if num_numeric>0 else None\n\n    def _cond_token(self, t, cat_indices_batch, fraud_bit_batch, do_cfg_dropout=True):\n        # t: [B], cat_indices_batch: [B, num_cat] corresponding to cat_cols_list\n        B = t.size(0)\n        # build cond embedding by concatenating embeddings of cond_columns and fraud bit\n        cond_parts = []\n        for c in self.cond_columns:\n            # find index of c in cat_cols_list\n            idx = self.cat_cols_list.index(c)\n            ids = cat_indices_batch[:, idx]  # [B]\n            cond_parts.append(self.cat_emb_modules[c](ids))  # [B, CAT_EMBED_DIM]\n        if len(cond_parts) > 0:\n            cond_cat_concat = torch.cat(cond_parts, dim=1)  # [B, len(cond_columns)*CAT_EMBED_DIM]\n        else:\n            cond_cat_concat = torch.zeros(B,0, device=t.device)\n        fraud_vec = fraud_bit_batch.view(B,1).to(t.device)  # [B,1]\n        cond_full = torch.cat([cond_cat_concat, fraud_vec], dim=1)  # [B, cond_input_dim]\n        # classifier-free guidance dropout\n        if self.training and do_cfg_dropout and self.uncond_prob>0:\n            mask = (torch.rand(B, device=t.device) < self.uncond_prob).float().unsqueeze(1)\n            cond_in = (1.0 - mask) * cond_full\n        else:\n            cond_in = cond_full\n        t_emb = fourier_time_embed(t, self.time_embed_dim)\n        t_proj = self.time_proj(t_emb)\n        c_proj = self.cond_proj(cond_in)\n        cond_tok = torch.cat([t_proj, c_proj], dim=1)\n        return self.cond_ln(cond_tok)  # [B, token_dim]\n\n    def _build_tokens_from_inputs(self, x_num, x_cat_indices, cond_token):\n        B = x_num.size(0)\n        toks = [cond_token.unsqueeze(1)]\n        # numeric tokens\n        if self.num_numeric>0:\n            num_tokens = []\n            for j in range(self.num_numeric):\n                token = self.num_proj[j](x_num[:, j:j+1])  # [B, token_dim]\n                num_tokens.append(token)\n            num_tokens = torch.stack(num_tokens, dim=1)\n            toks.append(num_tokens)\n        # categorical tokens: use shared cat_emb_modules\n        if self.num_cat>0:\n            cat_tokens = []\n            for j, c in enumerate(self.cat_cols_list):\n                ids = x_cat_indices[:, j]\n                cat_tokens.append(self.cat_emb_modules[c](ids))  # [B, token_dim]\n            cat_tokens = torch.stack(cat_tokens, dim=1)\n            toks.append(cat_tokens)\n        tokens = torch.cat(toks, dim=1)  # [B, 1+Tfeat, token_dim]\n        return tokens\n\n    def _alpha_sigma(self, t):\n        B = t.size(0)\n        t = t.view(B,1,1)\n        sigma = torch.exp(0.5 * (t * self.log_var_tokens.unsqueeze(0)))\n        alpha = torch.sqrt(torch.clamp(1.0 - sigma**2, min=1e-6))\n        return alpha, sigma\n\n    def training_loss(self, x_num, x_cat_indices, fraud_bits):\n        # x_num: [B, F], x_cat_indices: [B, C], fraud_bits: [B]\n        B = x_num.size(0)\n        device_local = x_num.device\n        t = torch.rand(B, device=device_local)\n        cond_token = self._cond_token(t, x_cat_indices, fraud_bits, do_cfg_dropout=True)\n        tokens_clean = self._build_tokens_from_inputs(x_num, x_cat_indices, cond_token)\n        cond_part = tokens_clean[:, :1, :]\n        feat_part = tokens_clean[:, 1:, :]\n        alpha, sigma = self._alpha_sigma(t)\n        eps = torch.randn_like(feat_part)\n        feat_noisy = alpha * feat_part + sigma * eps\n        tokens_noisy = torch.cat([cond_part, feat_noisy], dim=1)\n        den = self.transformer(tokens_noisy)\n        eps_hat = self.noise_head(den[:,1:,:])\n        loss = F.mse_loss(eps_hat, eps)\n        return loss\n\n    @torch.no_grad()\n    def sample(self, batch_size, cond_cat_indices, fraud_bits, steps=50, guidance_w=1.5, device_local=None):\n        # cond_cat_indices: [B, C] indices used for cond columns ONLY (we will broadcast if single)\n        device_local = device_local or next(self.parameters()).device\n        B = batch_size\n        if cond_cat_indices.size(0) == 1 and B > 1:\n            cond_cat_indices = cond_cat_indices.expand(B, -1).to(device_local)\n        fraud_bits = fraud_bits.view(-1,1).to(device_local) if fraud_bits.dim()==1 else fraud_bits.to(device_local)\n        t_grid = torch.linspace(1.0, 0.0, steps+1, device=device_local)\n        cond_tok = self._cond_token(torch.full((B,),1.0, device=device_local), cond_cat_indices, fraud_bits, do_cfg_dropout=False)\n        feat_tok = torch.randn(B, self.num_numeric + self.num_cat, self.token_dim, device=device_local)\n        # sampler loop (DDIM-like simplified)\n        for s in range(steps):\n            t_cur = t_grid[s].repeat(B)\n            t_next = t_grid[s+1].repeat(B)\n            alpha_cur, sigma_cur = self._alpha_sigma(t_cur)\n            alpha_next, sigma_next = self._alpha_sigma(t_next)\n            tokens = torch.cat([cond_tok.unsqueeze(1), feat_tok], dim=1)\n            den = self.transformer(tokens)\n            eps_c = self.noise_head(den[:,1:,:])\n            # build uncond\n            cond_tok_un = self._cond_token(t_cur, cond_cat_indices*0, fraud_bits*0, do_cfg_dropout=False)\n            tokens_u = torch.cat([cond_tok_un.unsqueeze(1), feat_tok], dim=1)\n            den_u = self.transformer(tokens_u)\n            eps_u = self.noise_head(den_u[:,1:,:])\n            eps = (1+guidance_w)*eps_c - guidance_w*eps_u\n            x0_est = (feat_tok - sigma_cur*eps) / (alpha_cur + 1e-6)\n            feat_tok = alpha_next * x0_est + sigma_next * eps\n            # decode periodically (simple deterministic)\n            if (s % 5) == 0:\n                tok_num = feat_tok[:, :self.num_numeric, :]\n                tok_cat = feat_tok[:, self.num_numeric:, :]\n                # decode cats\n                cat_ids = []\n                for j, dec in enumerate(self.cat_decoders):\n                    logits = dec(tok_cat[:, j, :])\n                    cat_ids.append(torch.argmax(logits, dim=-1))\n                cat_ids = torch.stack(cat_ids, dim=1)\n                # re-embed\n                re_emb = []\n                for j, c in enumerate(self.cat_cols_list):\n                    re_emb.append(self.cat_emb_modules[c](cat_ids[:, j]))\n                re_emb = torch.stack(re_emb, dim=1)\n                feat_tok = torch.cat([tok_num, re_emb], dim=1)\n        # final readout\n        tok_num = feat_tok[:, :self.num_numeric, :]\n        tok_cat = feat_tok[:, self.num_numeric:, :]\n        if self.num_numeric>0:\n            nums = []\n            for j, proj in enumerate(self.num_readout):\n                nums.append(proj(tok_num[:, j, :]).squeeze(-1))\n            x_num_out = torch.stack(nums, dim=1)\n        else:\n            x_num_out = torch.zeros(B,0, device=device_local)\n        cat_ids_out = []\n        for j, dec in enumerate(self.cat_decoders):\n            logits = dec(tok_cat[:, j, :])\n            cat_ids_out.append(torch.argmax(logits, dim=-1))\n        x_cat_out = torch.stack(cat_ids_out, dim=1) if len(cat_ids_out)>0 else torch.zeros(B,0, dtype=torch.long, device=device_local)\n        return x_num_out, x_cat_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate generator with shared cat_emb_modules\nFNUM = len(num_cols)\nCAT_COLS_LIST = cat_cols  # order matters: this list is used by the model to map indices -> embeddings\nCOND_COLS = cond_columns\n\ngen = TabDiffusionGeneratorRefactor(num_numeric=FNUM, cat_cols_list=CAT_COLS_LIST,\n                                    cat_emb_modules=cat_emb_modules, cond_columns=COND_COLS,\n                                    token_dim=192, transformer_layers=4, transformer_heads=4, transformer_ff=512,\n                                    uncond_prob=0.1).to(device)\n\n# move cat_emb_modules to device (they are referenced inside model but also separate ModuleDict)\ncat_emb_modules.to(device)\nprint(\"Generator and shared embeddings ready on device.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Test TabDiffusion with a real benchmark (IEEE-CIS)**","metadata":{}},{"cell_type":"markdown","source":"[The IEEE-CIS data](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203)\n\n\n**Load the input data**. In this case, we have two datasets: the transaction data and the identity data. \n\nEach row in the transaction data represents attributes of a specific transaction such as money transfer and also other gifting goods and service, like you booked a ticket for others, etc. and captures useful details such as transaction time, amount, payment card information, such as card type, card category, issue bank, country, etc, product or service procured (airtime, data, cable, ticket e.t.c) address, how many addresses are found to be associated with the payment card, distances between billing address, mailing address, zip code, IP address, phone area, etc. purchaser and recipient email domain, days between previous transaction, e.t.c, days between previous transaction, match, such as names on card and address, \n\nEach row in the identity dataset represents identity information such as network connection information (IP, ISP, Proxy, etc) and digital signature (UA/browser/os/version, etc) associated with each transaction. Also, it recorded behavioral fingerprints like account login times/failed to login times, how long an account stayed on the page, etc. \n\nThe transaction_id column is the unique identifier that relates both the transaction and identity tables.","metadata":{}},{"cell_type":"code","source":"# Load datasets\ntransaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\nidentity   = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\nprint(transaction.shape, identity.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# merge datasets\ndf = transaction.merge(identity, on='TransactionID', how='left')\nprint(\"Merged shape:\", df.shape)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Preprocess the Dataset** by dropping irrelevant columns, dealing with the missing values, scaling the numerical columns, and encoding the categorical columns","metadata":{}},{"cell_type":"code","source":"# drop identifier columns (we won't model\ndrop_cols = ['TransactionID']  # keep TransactionDT for time_to_detect\ncandidate = df.drop(columns=drop_cols)\n\n# identify label\nlabel_col = 'isFraud'\n\n# numeric / categorical heuristics\nnum_cols = candidate.select_dtypes(include=['int64','float64']).columns.tolist()\nnum_cols = [c for c in num_cols if c != label_col]\ncat_cols = candidate.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numeric cols:\", len(num_cols))\nprint(\"Categorical cols:\", len(cat_cols))\n# Optionally prune columns to avoid very high-cardinality ones (or group rare categories)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NAs\ndf[num_cols] = df[num_cols].fillna(0.0)\ndf[cat_cols] = df[cat_cols].fillna('NA')\n\n# scalers & encoders\nscaler = StandardScaler()\nscaler.fit(df[num_cols].values)   # save for inference\n\nlabel_encoders = {}\ncat_cardinalities = []\nfor c in cat_cols:\n    le = LabelEncoder()\n    df[c] = le.fit_transform(df[c].astype(str).values)\n    label_encoders[c] = le\n    cat_cardinalities.append(len(le.classes_))\n\nprint(\"cat_cardinalities:\", cat_cardinalities[:10])\nplt.hist(cat_cardinalities)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Define Conditional columns:**\n\nPick and instantiate candidate conditioning variables. These are the categorical features that are mostly useful in depicting the fraud types and provide context to attributes that are associated with certain fraud types, e.g., device used, account type, product type. These columns are fed into the conditional embedding vector (cond_vec), which the diffusion generator uses as side information. This forces the generator to learn distributions conditioned on context, e.g., Fraud patterns when using Device A vs Device B or under different product categories, as against just primarily learning distributions that are fraud against non-fraud. \nThis helps the generator capture multi-modal fraud patterns. It also gives us more control: later, when sampling, we can say, “Generate fraud-like transactions for DeviceType = mobile”, such that the generator will generate different fraud/non-fraud distributions depending on such device type. So in this case, the generator can generate synthetic data of fraudulent samples when the device is \"mobile\" and non-fraudulent samples when the device is also \"mobile\". It is unable to learn this level of representation without conditioning. This is important because fraud is rarely a uniform cluster — it often manifests differently depending on context (e.g., fraud on mobile devices, fraud in online purchases, fraud with high transaction amounts). With only \"isFraud\", the generator has no way to control which type of fraud it’s generating. So these conditioning columns help the generator learn subtypes of fraud (more adversarial realism, but more complexity).","metadata":{}},{"cell_type":"code","source":"# Choose a handful of columns to form a conditioner (identity/device + product)\ncond_columns = [\n    'ProductCD', 'card4', 'card6',\n    'DeviceType', 'DeviceInfo',\n    'id_30', 'id_31',\n    'addr1', 'P_emaildomain', 'R_emaildomain'\n]\ncond_columns = [c for c in cond_columns if c in df.columns]\nprint(\"Using conditional columns:\", cond_columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Build embedding lookup dictionary module** \n\nCreates an embedding table that stores the embedding of each conditioning variable with shape [n_categories, embedding_dim], where n_categories is the number of labels in each class and embedding_dim is the set embedding dimension (32 in this case); Such that each category (like visa or Windows 10) gets mapped to a learnable 32-dimensional vector and stored in a dictionary keyed by column name that allows us look up the embeddings column by column during the forward pass.\n\n","metadata":{}},{"cell_type":"code","source":"# Make embedding layers for categorical columns and concatenate\nCAT_EMBED_DIM = 32\ncat_emb_modules = nn.ModuleDict()\nfor c, card in cat_cardinalities.items():\n    cat_emb_modules[c] = nn.Embedding(card, CAT_EMBED_DIM)\nprint(\"Built shared cat_emb_modules for\", len(cat_emb_modules), \"categorical columns.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset - returns numeric features, categorical indices, label, time, account id\nclass IEEECISDatasetTorch(Dataset):\n    def __init__(self, df, num_cols, cat_cols, label_col='isFraud', acct_col='card1', scaler=None):\n        self.df = df.reset_index(drop=True)\n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label_col = label_col\n        self.acct_col = acct_col\n        self.scaler = scaler\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        if self.num_cols:\n            x_num = torch.tensor(self.scaler.transform(row[self.num_cols].values.reshape(1,-1)).squeeze(0), dtype=torch.float32)\n        else:\n            x_num = torch.zeros(0, dtype=torch.float32)\n        x_cat = torch.tensor(row[self.cat_cols].astype(int).values, dtype=torch.long) if self.cat_cols else torch.zeros(0, dtype=torch.long)\n        y = torch.tensor(int(row[self.label_col]) if self.label_col in row else 0, dtype=torch.long)\n        tx_time = row['TransactionDT'] if 'TransactionDT' in row else np.nan\n        acct = row[self.acct_col] if self.acct_col in row else None\n        return x_num, x_cat, y, tx_time, acct\n\ndef collate_fn(batch):\n    x_num = torch.stack([b[0] for b in batch], dim=0)\n    x_cat = torch.stack([b[1] for b in batch], dim=0)\n    y = torch.stack([b[2] for b in batch], dim=0)\n    tx_time = [b[3] for b in batch]\n    acct = [b[4] for b in batch]\n    return x_num, x_cat, y, tx_time, acct\n\n# Create DataLoader\ndataset = IEEECISDatasetTorch(df, num_cols, cat_cols, label_col=LABEL, acct_col='card1', scaler=scaler)\nloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\nprint(\"Dataset and loader ready. Dataset size:\", len(dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Smoke test: run one training step on a single batch from the loader\nimport torch.optim as optim\nopt = optim.AdamW(gen.parameters(), lr=2e-4)\n\n# fetch a batch\nbatch = next(iter(loader))\nx_num, x_cat, y, tx_time, acct = batch\nx_num = x_num.to(device); x_cat = x_cat.to(device); y = y.to(device).float()\n\ngen.train()\nloss = gen.training_loss(x_num, x_cat, y)\nopt.zero_grad()\nloss.backward()\nopt.step()\nprint('Smoke step loss:', loss.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sampling example: pick one row as a condition and generate both non-fraud and fraud samples\ngen.eval()\n# pick first row as condition\nrow_idx = 0\n_, x_cat_row, y_row, _, _ = dataset[row_idx]\n# build cond indices for cond_columns (we need indices aligned to cat_cols order)\n# cond_cat_indices is of shape [1, len(cat_cols)], but model expects all cat_cols; we'll pass full x_cat row\ncond_cat_indices = x_cat_row.unsqueeze(0).to(device)\nfraud_bit_0 = torch.tensor([0.0], device=device)\nfraud_bit_1 = torch.tensor([1.0], device=device)\n\nwith torch.no_grad():\n    num_gen_0, cat_gen_0 = gen.sample(batch_size=8, cond_cat_indices=cond_cat_indices, fraud_bits=fraud_bit_0, steps=30, guidance_w=1.2, device_local=device)\n    num_gen_1, cat_gen_1 = gen.sample(batch_size=8, cond_cat_indices=cond_cat_indices, fraud_bits=fraud_bit_1, steps=30, guidance_w=1.2, device_local=device)\n\nprint('Generated non-fraud numeric shape, cat shape:', num_gen_0.shape, cat_gen_0.shape)\nprint('Generated fraud     numeric shape, cat shape:', num_gen_1.shape, cat_gen_1.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}