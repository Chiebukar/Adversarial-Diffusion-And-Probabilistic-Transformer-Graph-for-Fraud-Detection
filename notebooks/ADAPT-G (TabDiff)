{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ADAPT-G: Adversarial Diffusion And Probabilistic Transformer-Graph for Financial Fraud Detection**\n\n---\n\n## Overview\n\n**ADAPT-G** is a novel hybrid architecture designed for **financial transaction and mobile money fraud detection**.  \nIt combines **adversarial curriculum learning with a diffusion-based generator** to create realistic synthetic fraud scenarios and a **probabilistic Transformer-Graph Neural Network (GNN) detector** to identify both **behavioral** (temporal) and **structural** (relational) anomalies.\n\nOur goal is to **push the boundaries of fraud detection** by addressing the weaknesses of traditional models in catching **stealthy, low-signal fraud patterns** — particularly those that exploit the **network relationships** between entities and **temporal transaction patterns**.\n\n---\n\n## Research Goals\n\n- **Develop** a hybrid adversarial architecture capable of detecting complex fraud signals in transaction networks.\n- **Incorporate** probabilistic reasoning into the detection stage for better uncertainty estimation and high-risk flagging.\n- **Benchmark** against state-of-the-art fraud detection models on real-world financial datasets.\n- **Evaluate** across both temporal and graph-based perspectives to capture full fraud signatures.\n\n---\n\n## Model Architecture\n\nThe **ADAPT-G** framework has two major components:\n\n1. **Diffusion-based Generator**  \n   - Generates high-quality, diverse fraudulent transaction sequences and relational patterns.  \n   - Uses **adversarial curriculum learning** to gradually create harder-to-detect fraudulent scenarios.\n\n2. **Probabilistic Transformer-GNN Detector**  \n   - **Transformer**: Models per-entity temporal transaction behavior.\n   - **Lightweight GNN**: Captures graph-level relational dependencies between entities (accounts, devices, merchants).  \n   - **Probabilistic Output Layer**: Produces calibrated uncertainty scores alongside classification outputs.\n\n---\n\n## Benchmark Models\n\nWe will compare ADAPT-G against:\n\n- **XGBoost** (tabular, feature-engineered baseline)\n- **GCN** (Graph Convolutional Network)\n- **GAT** (Graph Attention Network)\n- **Temporal GNNs** (TGAT, TGN)\n- **LSTM/GRU Autoencoders** (temporal anomaly detection baselines)\n- **TabTransformer** (tabular deep learning baseline)\n\n---\n\n## Datasets\n\nWe will evaluate on **multiple benchmark datasets** for financial/mobile money fraud detection:\n\n- **IEEE-CIS Fraud Detection** (Kaggle)\n- **PaySim** (Mobile money fraud simulation)\n- **Elliptic Dataset** (Bitcoin transaction fraud/illicit activity)\n- **SIMF Dataset** (Mobile money & transaction fraud – simulated and real-world mix)\n\nEach dataset will be preprocessed into both **tabular** and **graph-structured formats** where applicable, ensuring fair comparison across all models.\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Imports & Utilities**\n\n---\n\nThis section loads all **core libraries** required for the ADAPT-G framework and defines key **utility functions** for:\n\n1. **Seeding for reproducibility** — Ensures consistent experimental results.\n2. **Metric calculations** — Precision, Recall, F1-score, AUC-ROC, and more.\n3. **Visualization** — For training/evaluation curves, ROC plots, and confusion matrices.\n","metadata":{}},{"cell_type":"code","source":"# # Install required packages\n# !pip install torch torchvision torch_geometric transformers diffusers scikit-learn pandas matplotlib seaborn tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Imports\n# ================================\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix,\n    accuracy_score\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, GATConv, global_mean_pool\nfrom datetime import datetime","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Helper Functions\n# ================================\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Sets seed for reproducibility across Python, NumPy, and PyTorch.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"[INFO] Seed set to: {seed}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Metrics\n# -------------------------------------------------\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    \"\"\"\n    Calculate core classification metrics.\n    y_true: Ground truth labels\n    y_pred: Predicted labels\n    y_prob: Predicted probabilities (for ROC/AUC)\n    \"\"\"\n    metrics_dict = {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n    }\n    if y_prob is not None:\n        metrics_dict[\"AUC-ROC\"] = roc_auc_score(y_true, y_prob)\n    return metrics_dict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Visualization Utilities\n# -------------------------------------------------\ndef plot_loss_curves(train_losses, val_losses):\n    plt.figure(figsize=(8, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training & Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_roc_curve(y_true, y_prob):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    auc_score = roc_auc_score(y_true, y_prob)\n    plt.figure(figsize=(6, 6))\n    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, labels=None):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Model Architecture**\n\nThis section defines the two core components of **ADAPT-G**:\n\n---\n\n### **3.1 Generator — TabDiffusion Sample Simulator**  \nWe use **TabDiffusion** to generate **both fraudulent and non-fraudulent transaction samples**.  \n\n- **Conditional Generation:** Generator is conditioned on a label (`fraud = 0` or `1`) along with context features (merchant, device, account profile).  \n- **Dual Simulation:**  \n  - Fraudulent samples: Rare anomalies, generated to mimic realistic fraud signatures.  \n  - Non-fraudulent samples: Plausible “normal” transactions to maintain balance and realism.  \n- **Adversarial Curriculum:** Generator evolves to create increasingly **hard-to-discriminate samples**, forcing the detector to improve.  \n\n> Unlike standard anomaly generation approaches, TabDiffusion provides **balanced synthetic data augmentation** for both classes, addressing fraud rarity while avoiding biased training.  \n\n---\n\n### **3.2 Detector — Probabilistic Transformer-GNN**  \nThe detector integrates **graph structure** and **temporal behavior** to validate whether a given transaction is fraudulent or not:  \n\n- **Graph Encoder (GNN):** Captures structural dependencies across accounts, devices, and merchants.  \n- **Transformer Encoder:** Models sequential patterns in entity transaction histories.  \n- **Fusion Layer:** Combines graph + temporal embeddings.  \n- **Probabilistic Output Layer:**  \n  - Produces fraud classification (fraud vs. non-fraud).  \n  - Estimates prediction uncertainty via **MC Dropout**.  \n\n---\n\n### **3.3 Joint Training Paradigm**  \n- Generator (TabDiffusion) produces both **fraudulent and non-fraudulent samples**.  \n- Detector (Transformer-GNN) learns to confirm whether transactions are truly fraudulent or not.  \n- Adversarial training loop ensures:  \n  - Generator creates challenging, realistic samples.  \n  - Detector improves robustness by distinguishing real vs synthetic and fraud vs non-fraud simultaneously.  ","metadata":{}},{"cell_type":"code","source":"# ============================================\n# Generator: TabDiffusion Sample Simulator\n# ============================================\n\n\nclass TabDiffusionGenerator(nn.Module):\n    \"\"\"\n    TabDiffusion-based conditional generator for fraud + non-fraud samples.\n    \n    This generator follows the DDPM paradigm but is adapted to tabular data:\n    - Learns to reverse a noise process and reconstruct samples.\n    - Can condition on both context (device/account/merchant profile) and fraud label.\n    - Generates both fraudulent and non-fraudulent samples.\n    \"\"\"\n    def __init__(self, input_dim, cond_dim, hidden_dim=256, num_steps=1000, beta_start=1e-4, beta_end=0.02):\n        super().__init__()\n        \n        # ------------------------\n        # Diffusion Hyperparameters\n        # ------------------------\n        self.num_steps = num_steps\n        self.register_buffer(\"betas\", torch.linspace(beta_start, beta_end, num_steps))\n        self.register_buffer(\"alphas\", 1.0 - self.betas)\n        self.register_buffer(\"alpha_bars\", torch.cumprod(self.alphas, dim=0))\n        \n        # ------------------------\n        # Conditional Denoising Network\n        # ------------------------\n        # Takes noisy sample + condition vector (context + fraud label)\n        self.denoise_net = nn.Sequential(\n            nn.Linear(input_dim + cond_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim)\n        )\n    \n    def forward_diffusion(self, x0, t):\n        \"\"\"\n        Forward diffusion: add noise to the clean sample x0 at step t.\n        \"\"\"\n        noise = torch.randn_like(x0)\n        sqrt_alpha_bar = torch.sqrt(self.alpha_bars[t]).unsqueeze(-1)\n        sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bars[t]).unsqueeze(-1)\n        xt = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n        return xt, noise\n    \n    def reverse_denoise(self, xt, t, cond_vec):\n        \"\"\"\n        Reverse step: predict noise from noisy sample + condition.\n        \"\"\"\n        cond_input = torch.cat([xt, cond_vec], dim=-1)\n        noise_pred = self.denoise_net(cond_input)\n        return noise_pred\n    \n    def sample(self, cond_vec, device=\"cpu\"):\n        \"\"\"\n        Generate a new transaction sample conditioned on context + fraud label.\n        \n        Args:\n            cond_vec (Tensor): Conditioning vector [batch_size, cond_dim].\n            device (str): Device to run sampling on.\n        \n        Returns:\n            Tensor: Generated samples [batch_size, input_dim].\n        \"\"\"\n        batch_size = cond_vec.size(0)\n        x = torch.randn(batch_size, self.denoise_net[-1].out_features, device=device)\n        \n        for t in reversed(range(self.num_steps)):\n            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n            \n            # Predict noise\n            noise_pred = self.reverse_denoise(x, t_tensor, cond_vec)\n            \n            # Update sample\n            alpha = self.alphas[t]\n            alpha_bar = self.alpha_bars[t]\n            beta = self.betas[t]\n            \n            # DDPM update rule\n            if t > 0:\n                noise = torch.randn_like(x)\n            else:\n                noise = 0\n            \n            x = (1 / torch.sqrt(alpha)) * (x - (1 - alpha) / torch.sqrt(1 - alpha_bar) * noise_pred) + torch.sqrt(beta) * noise\n        \n        return x\n    \n    def training_loss(self, x0, cond_vec, device=\"cpu\"):\n        \"\"\"\n        Compute training loss for the diffusion model.\n        \"\"\"\n        batch_size = x0.size(0)\n        t = torch.randint(0, self.num_steps, (batch_size,), device=device).long()\n        \n        # Forward diffusion\n        xt, noise = self.forward_diffusion(x0, t)\n        \n        # Predict noise\n        noise_pred = self.reverse_denoise(xt, t, cond_vec)\n        \n        # Loss = MSE between predicted and true noise\n        return F.mse_loss(noise_pred, noise)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}