{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **ADAPT-G: Adversarial Diffusion And Probabilistic Transformer-Graph for Financial Fraud Detection**\n\n---\n\n## Overview\n\n**ADAPT-G** is a novel hybrid architecture designed for **financial transaction and mobile money fraud detection**.  \nIt combines **adversarial curriculum learning with a diffusion-based generator** to create realistic synthetic fraud scenarios and a **probabilistic Transformer-Graph Neural Network (GNN) detector** to identify both **behavioral** (temporal) and **structural** (relational) anomalies.\n\nOur goal is to **push the boundaries of fraud detection** by addressing the weaknesses of traditional models in catching **stealthy, low-signal fraud patterns** — particularly those that exploit the **network relationships** between entities and **temporal transaction patterns**.\n\n---\n\n## Research Goals\n\n- **Develop** a hybrid adversarial architecture capable of detecting complex fraud signals in transaction networks.\n- **Incorporate** probabilistic reasoning into the detection stage for better uncertainty estimation and high-risk flagging.\n- **Benchmark** against state-of-the-art fraud detection models on real-world financial datasets.\n- **Evaluate** across both temporal and graph-based perspectives to capture full fraud signatures.\n\n---\n\n## Model Architecture\n\nThe **ADAPT-G** framework has two major components:\n\n1. **Diffusion-based Generator**  \n   - Generates high-quality, diverse fraudulent transaction sequences and relational patterns.  \n   - Uses **adversarial curriculum learning** to gradually create harder-to-detect fraudulent scenarios.\n\n2. **Probabilistic Transformer-GNN Detector**  \n   - **Transformer**: Models per-entity temporal transaction behavior.\n   - **Lightweight GNN**: Captures graph-level relational dependencies between entities (accounts, devices, merchants).  \n   - **Probabilistic Output Layer**: Produces calibrated uncertainty scores alongside classification outputs.\n\n---\n\n## Benchmark Models\n\nWe will compare ADAPT-G against:\n\n- **XGBoost** (tabular, feature-engineered baseline)\n- **GCN** (Graph Convolutional Network)\n- **GAT** (Graph Attention Network)\n- **Temporal GNNs** (TGAT, TGN)\n- **LSTM/GRU Autoencoders** (temporal anomaly detection baselines)\n- **TabTransformer** (tabular deep learning baseline)\n\n---\n\n## Datasets\n\nWe will evaluate on **multiple benchmark datasets** for financial/mobile money fraud detection:\n\n- **IEEE-CIS Fraud Detection** (Kaggle)\n- **PaySim** (Mobile money fraud simulation)\n- **Elliptic Dataset** (Bitcoin transaction fraud/illicit activity)\n- **SIMF Dataset** (Mobile money & transaction fraud – simulated and real-world mix)\n\nEach dataset will be preprocessed into both **tabular** and **graph-structured formats** where applicable, ensuring fair comparison across all models.\n","metadata":{}},{"cell_type":"markdown","source":"## **2. Imports & Utilities**\n\n---\n\nThis section loads all **core libraries** required for the ADAPT-G framework and defines key **utility functions** for:\n\n1. **Seeding for reproducibility** — Ensures consistent experimental results.\n2. **Metric calculations** — Precision, Recall, F1-score, AUC-ROC, and more.\n3. **Visualization** — For training/evaluation curves, ROC plots, and confusion matrices.\n","metadata":{}},{"cell_type":"code","source":"# # Install required packages\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118  # adjust cu version or use cpu wheel\n!pip install torch-geometric torch-scatter torch-sparse torch-spline-conv torch-cluster --quiet  # follow torch_geometric install instructions for your torch version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T03:36:06.542825Z","iopub.execute_input":"2025-08-27T03:36:06.543107Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.2\nLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━\u001b[0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.820m [nvidia-nvjitlink-cu12]\n\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:0m \u001b[32m 0/10\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/10\u001b[0m [nvidia-curand-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/10\u001b[0m [nvidia-cufft-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/10\u001b[0m [nvidia-cuda-runtime-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/10\u001b[0m [nvidia-cuda-nvrtc-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/10\u001b[0m [nvidia-cuda-cupti-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cusparse-cu120m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/10\u001b[0m [nvidia-cublas-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m 7/10\u001b[0m [nvidia-cusparse-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.750m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m 8/10\u001b[0m [nvidia-cudnn-cu12]\n\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.830m━━━\u001b[0m \u001b[32m 9/10\u001b[0m [nvidia-cusolver-cu12]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [nvidia-cusolver-cu12]dia-cusolver-cu12]\n\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: Building 'torch-scatter' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torch-scatter'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: Building 'torch-sparse' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torch-sparse'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[33m  DEPRECATION: Building 'torch-spline-conv' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'torch-spline-conv'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Imports\n# ================================\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve, confusion_matrix,\n    accuracy_score\n)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_geometric\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GCNConv, GATConv, global_mean_pool\nfrom datetime import datetime\n\n\nimport math\nfrom typing import List, Optional, Tuple\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================\n# Helper Functions\n# ================================\n\ndef set_seed(seed: int = 42):\n    \"\"\"\n    Sets seed for reproducibility across Python, NumPy, and PyTorch.\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(f\"[INFO] Seed set to: {seed}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Metrics\n# -------------------------------------------------\ndef calculate_metrics(y_true, y_pred, y_prob=None):\n    \"\"\"\n    Calculate core classification metrics.\n    y_true: Ground truth labels\n    y_pred: Predicted labels\n    y_prob: Predicted probabilities (for ROC/AUC)\n    \"\"\"\n    metrics_dict = {\n        \"Accuracy\": accuracy_score(y_true, y_pred),\n        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n        \"F1\": f1_score(y_true, y_pred, zero_division=0)\n    }\n    if y_prob is not None:\n        metrics_dict[\"AUC-ROC\"] = roc_auc_score(y_true, y_prob)\n    return metrics_dict\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------\n# Visualization Utilities\n# -------------------------------------------------\ndef plot_loss_curves(train_losses, val_losses):\n    plt.figure(figsize=(8, 5))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training & Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_roc_curve(y_true, y_prob):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    auc_score = roc_auc_score(y_true, y_prob)\n    plt.figure(figsize=(6, 6))\n    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.3f}')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.show()\n\ndef plot_confusion_matrix(y_true, y_pred, labels=None):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Model Architecture**\n\nThis section defines the two core components of **ADAPT-G**:\n\n---\n\n### **3.1 Generator — TabDiffusion Sample Simulator**  \nWe use **TabDiffusion** to generate **both fraudulent and non-fraudulent transaction samples**.  \n\n- **Conditional Generation:** Generator is conditioned on a label (`fraud = 0` or `1`) along with context features (merchant, device, account profile).  \n- **Dual Simulation:**  \n  - Fraudulent samples: Rare anomalies, generated to mimic realistic fraud signatures.  \n  - Non-fraudulent samples: Plausible “normal” transactions to maintain balance and realism.  \n- **Adversarial Curriculum:** Generator evolves to create increasingly **hard-to-discriminate samples**, forcing the detector to improve.  \n\n> Unlike standard anomaly generation approaches, TabDiffusion provides **balanced synthetic data augmentation** for both classes, addressing fraud rarity while avoiding biased training.  \n\n---\n\n### **3.2 Detector — Probabilistic Transformer-GNN**  \nThe detector integrates **graph structure** and **temporal behavior** to validate whether a given transaction is fraudulent or not:  \n\n- **Graph Encoder (GNN):** Captures structural dependencies across accounts, devices, and merchants.  \n- **Transformer Encoder:** Models sequential patterns in entity transaction histories.  \n- **Fusion Layer:** Combines graph + temporal embeddings.  \n- **Probabilistic Output Layer:**  \n  - Produces fraud classification (fraud vs. non-fraud).  \n  - Estimates prediction uncertainty via **MC Dropout**.  \n\n---\n\n### **3.3 Joint Training Paradigm**  \n- Generator (TabDiffusion) produces both **fraudulent and non-fraudulent samples**.  \n- Detector (Transformer-GNN) learns to confirm whether transactions are truly fraudulent or not.  \n- Adversarial training loop ensures:  \n  - Generator creates challenging, realistic samples.  \n  - Detector improves robustness by distinguishing real vs synthetic and fraud vs non-fraud simultaneously.  ","metadata":{}},{"cell_type":"markdown","source":"### **4.1 Generator: Tabular Diffusion**","metadata":{}},{"cell_type":"code","source":"# TabDiffusion-style mixed-type conditional generator (Transformer denoiser + mixed-type sampler)\n# Notebook-ready, experiment-ready implementation.\n# Author: (adapted for ADAPT-Fraud)\n# Requirements: torch >=1.12, (GPU highly recommended)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Utilities\n# -------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef fourier_time_embed(t: torch.Tensor, dim: int = 64) -> torch.Tensor:\n    \"\"\"Sinusoidal / Fourier time embedding for t in [0,1]. t: [B] -> out [B, dim].\"\"\"\n    half = dim // 2\n    freqs = torch.exp(torch.linspace(math.log(1.0), math.log(1000.0), half, device=t.device))\n    args = t.unsqueeze(-1) * freqs.unsqueeze(0)  # [B, half]\n    return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Tokenizer (numeric + categorical -> tokens)\n# -------------------------\nclass MixedTypeTokenizer(nn.Module):\n    \"\"\"\n    Build per-sample token sequence:\n      [COND] token (represents time+cond), then numeric tokens (one per numeric feature),\n      then categorical tokens (embedding output for each categorical column).\n    Inputs to training functions are:\n      - x_num: [B, Fnum] floats (normalized)\n      - x_cat: [B, C] longs (category indices)\n      - cond_vec: [B, Dcond] floats (embedding or concatenated context, includes fraud label)\n    \"\"\"\n    def __init__(self, num_numeric: int, cat_cardinalities: List[int], token_dim: int = 256, use_cond_token: bool = True):\n        super().__init__()\n        self.num_numeric = num_numeric\n        self.cat_cardinalities = cat_cardinalities\n        self.num_cat = len(cat_cardinalities)\n        self.token_dim = token_dim\n        self.use_cond_token = use_cond_token\n\n        # numeric projection: map 1-d numeric to token_dim (separate linear for each numeric feature)\n        # implement as a common Linear applied per-feature for simplicity (works well if values normalized)\n        self.num_proj = nn.Linear(1, token_dim) if num_numeric > 0 else None\n\n        # categorical embeddings per column\n        self.cat_embs = nn.ModuleList([nn.Embedding(card, token_dim) for card in cat_cardinalities])\n\n        # total token count excluding [COND]\n        self.num_feat_tokens = num_numeric + self.num_cat\n        # cond token created outside from cond_vec + t embedding\n\n    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor], cond_token: Optional[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Returns tokens: [B, 1 + num_feat_tokens, token_dim] if use_cond_token True,\n                         else [B, num_feat_tokens, token_dim]\n        cond_token: [B, token_dim] (already time+cond projected)\n        \"\"\"\n        toks = []\n        B = None\n        if self.use_cond_token:\n            assert cond_token is not None\n            toks.append(cond_token.unsqueeze(1))  # [B,1,token_dim]\n            B = cond_token.size(0)\n        else:\n            B = x_num.size(0) if x_num is not None else x_cat.size(0)\n\n        if self.num_numeric > 0 and x_num is not None:\n            # project each scalar numeric into a token; we'll apply num_proj to each column slice\n            # x_num: [B, Fnum]\n            num_tokens = []\n            for j in range(self.num_numeric):\n                # shape [B,1]\n                col = x_num[:, j:j+1]\n                num_tokens.append(self.num_proj(col))  # [B, token_dim]\n            num_tokens = torch.stack(num_tokens, dim=1)  # [B, Fnum, token_dim]\n            toks.append(num_tokens)\n\n        if self.num_cat > 0 and x_cat is not None:\n            cat_tokens = []\n            for j, emb in enumerate(self.cat_embs):\n                cat_tokens.append(emb(x_cat[:, j]))  # [B, token_dim]\n            cat_tokens = torch.stack(cat_tokens, dim=1)  # [B, C, token_dim]\n            toks.append(cat_tokens)\n\n        return torch.cat(toks, dim=1)  # [B, T, token_dim]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# Transformer denoiser (token-level)\n# -------------------------\nclass TransformerDenoiser(nn.Module):\n    def __init__(self, token_dim: int = 256, nhead: int = 8, nlayers: int = 6, ff_dim: int = 1024, dropout: float = 0.1):\n        super().__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=token_dim, nhead=nhead, dim_feedforward=ff_dim, dropout=dropout, activation='gelu', batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n        self.ln = nn.LayerNorm(token_dim)\n\n    def forward(self, tokens: torch.Tensor, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # tokens: [B, T, token_dim]\n        out = self.transformer(tokens, src_key_padding_mask=src_key_padding_mask)\n        out = self.ln(out)\n        return out  # [B, T, token_dim]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------\n# TabDiffusion generator class\n# -------------------------\nclass TabDiffusionGenerator(nn.Module):\n    \"\"\"\n    TabDiffusion-style mixed-type conditional generator.\n\n    Inputs expected for training:\n      - x_num: [B, Fnum] (normalized continuous)\n      - x_cat: [B, C] (long indices)\n      - cond_vec: [B, Dcond] (float tensor; should include fraud label and context)\n    Key API:\n      - training_loss(x_num, x_cat, cond_vec) -> scalar loss\n      - sample(batch_size, cond_vec, steps=..., guidance=..., detector=None, det_guidance_w=0.0) -> x_num_gen, x_cat_gen\n    \"\"\"\n    def __init__(\n        self,\n        num_numeric: int,\n        cat_cardinalities: List[int],\n        cond_dim: int,\n        token_dim: int = 256,\n        time_embed_dim: int = 64,\n        transformer_layers: int = 6,\n        transformer_heads: int = 8,\n        transformer_ff: int = 1024,\n        uncond_prob: float = 0.1,   # classifier-free guidance drop probability\n        p_refresh: float = 0.15     # default refresh prob during sampling\n    ):\n        super().__init__()\n        self.num_numeric = num_numeric\n        self.cat_cardinalities = cat_cardinalities\n        self.num_cat = len(cat_cardinalities)\n        self.token_dim = token_dim\n        self.cond_dim = cond_dim\n        self.time_embed_dim = time_embed_dim\n        self.uncond_prob = uncond_prob\n        self.p_refresh = p_refresh\n\n        # tokenizer: produces [COND] + feature tokens\n        self.tokenizer = MixedTypeTokenizer(num_numeric=num_numeric, cat_cardinalities=cat_cardinalities, token_dim=token_dim, use_cond_token=True)\n\n        # time and cond projections to produce COND token\n        self.time_proj = nn.Sequential(nn.Linear(time_embed_dim, token_dim//2), nn.GELU(), nn.Linear(token_dim//2, token_dim//2))\n        self.cond_proj = nn.Sequential(nn.Linear(cond_dim, token_dim//2), nn.GELU(), nn.Linear(token_dim//2, token_dim//2))\n        self.cond_layernorm = nn.LayerNorm(token_dim)\n\n        # Transformer denoiser\n        self.denoiser = TransformerDenoiser(token_dim=token_dim, nhead=transformer_heads, nlayers=transformer_layers, ff_dim=transformer_ff)\n\n        # per-feature learnable log-variance at t=1.0 (token-level)\n        total_feat_tokens = num_numeric + self.num_cat\n        # we store per-token, per-dim log_var (token dimensionality)\n        self.log_var_tokens = nn.Parameter(torch.zeros(total_feat_tokens, token_dim))  # learnable schedule endpoint\n\n        # shared head to predict token noise; applied to feature tokens only (not COND)\n        self.noise_head = nn.Linear(token_dim, token_dim)\n\n        # categorical decoders (project token -> logits)\n        self.cat_decoders = nn.ModuleList([nn.Linear(token_dim, card) for card in cat_cardinalities])\n\n        # numeric readout head: token_dim -> scalar numeric prediction per numeric token\n        self.num_readout = nn.ModuleList([nn.Linear(token_dim, 1) for _ in range(num_numeric)]) if num_numeric > 0 else None\n\n    # -----------\n    # Internal helpers\n    # -----------\n    def _cond_token(self, t: torch.Tensor, cond_vec: torch.Tensor, do_cfg_dropout: bool = True) -> torch.Tensor:\n        \"\"\"\n        Build [COND] token from t (continuous time in [0,1]) and cond_vec (B,cond_dim).\n        If training and uncond_prob>0, randomly drop cond_vec for classifier-free guidance.\n        Returns: [B, token_dim]\n        \"\"\"\n        B = t.size(0)\n        # classifier-free guidance dropout\n        if self.training and do_cfg_dropout and self.uncond_prob > 0:\n            mask = (torch.rand(B, device=t.device) < self.uncond_prob).float().unsqueeze(1)  # [B,1]\n            cond_in = (1.0 - mask) * cond_vec\n        else:\n            cond_in = cond_vec\n        t_emb = fourier_time_embed(t, self.time_embed_dim)  # [B, time_embed_dim]\n        t_p = self.time_proj(t_emb)                         # [B, token_dim//2]\n        c_p = self.cond_proj(cond_in)                       # [B, token_dim//2]\n        cond_tok = torch.cat([t_p, c_p], dim=-1)            # [B, token_dim]\n        return self.cond_layernorm(cond_tok)\n\n    def _alpha_sigma(self, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Continuous-time per-token alpha(t), sigma(t).\n        Using simple parametrization: sigma(t) = exp(0.5 * t * log_var_token)\n        alpha = sqrt(1 - sigma^2)\n        t: [B] in [0,1], returns:\n          alpha: [B, Tfeat, token_dim]\n          sigma: [B, Tfeat, token_dim]\n        \"\"\"\n        B = t.size(0)\n        # log_var_tokens: [Tfeat, token_dim]\n        t = t.view(B, 1, 1)  # [B,1,1]\n        sigma = torch.exp(0.5 * (t * self.log_var_tokens.unsqueeze(0)))  # [B, Tfeat, token_dim]\n        alpha = torch.sqrt(torch.clamp(1.0 - sigma**2, min=1e-6))\n        return alpha, sigma\n\n    # -----------\n    # Forward (training) - produce noisy tokens and compute denoiser loss\n    # -----------\n    def training_loss(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor], cond_vec: torch.Tensor):\n        \"\"\"\n        Compute denoising loss for a batch.\n        - x_num: [B, Fnum] normalized numeric features\n        - x_cat: [B, C] categorical column indices\n        - cond_vec: [B, Dcond]\n        Returns: scalar loss and diagnostic dict\n        \"\"\"\n        B = cond_vec.size(0)\n        device_local = cond_vec.device\n        # sample continuous times uniformly in [0,1]\n        t = torch.rand(B, device=device_local)\n\n        # Build cond token (with CFG dropout inside)\n        cond_token = self._cond_token(t, cond_vec, do_cfg_dropout=True)  # [B, token_dim]\n\n        # Build clean tokens (COND + feature tokens)\n        tokens_clean = self.tokenizer(x_num, x_cat, cond_token)  # [B, 1+Tfeat, token_dim]\n        # Split\n        cond_part = tokens_clean[:, :1, :]   # [B,1,d]\n        feat_part = tokens_clean[:, 1:, :]   # [B, Tfeat, d]\n\n        # sample noise and noisy tokens for feature tokens\n        alpha, sigma = self._alpha_sigma(t)  # [B, Tfeat, d]\n        eps = torch.randn_like(feat_part)\n        feat_noisy = alpha * feat_part + sigma * eps\n        tokens_noisy = torch.cat([cond_part, feat_noisy], dim=1)  # [B, 1+Tfeat, d]\n\n        # Denoiser predicts noise for feature tokens (exclude cond token)\n        denoised = self.denoiser(tokens_noisy)  # [B, 1+Tfeat, d]\n        eps_hat = self.noise_head(denoised[:, 1:, :])  # [B, Tfeat, d]\n\n        # MSE loss between eps_hat and eps\n        loss = F.mse_loss(eps_hat, eps)\n\n        return loss, {'mse': loss.detach()}\n\n    # -----------\n    # Mixed-type stochastic sampler with classifier-free guidance (CFG)\n    # and optional detector-guidance (gradient-based)\n    # -----------\n    @torch.no_grad()\n    def sample(\n        self,\n        batch_size: int,\n        cond_vec: torch.Tensor,\n        steps: int = 60,\n        guidance_w: float = 2.0,\n        decode_every: int = 1,\n        p_refresh: float = 0.15,\n        refresh_sigma: float = 0.08,\n        stochastic_decode: bool = True,\n        detector_fn: Optional[callable] = None,\n        det_guidance_w: float = 0.0,\n        device_local: Optional[torch.device] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Sample mixed-type rows:\n          - cond_vec: [B, Dcond] (if B=batch_size else will be broadcast)\n          - detector_fn: optional callable(x_num:Tensor, x_cat:Tensor) -> scalar logits (higher = more likely fraud)\n                         Detector guidance will try to increase or decrease that objective depending on sign of det_guidance_w.\n        Returns:\n          - x_num_gen: [B, Fnum]\n          - x_cat_gen: [B, C] (long)\n        \"\"\"\n        self.eval()\n        device_local = device_local or cond_vec.device\n        B = batch_size\n        if cond_vec.size(0) != B:\n            # broadcast cond vec if single cond provided\n            cond_vec = cond_vec.expand(B, -1).to(device_local)\n\n        # time grid t from 1.0 -> 0.0\n        t_grid = torch.linspace(1.0, 0.0, steps + 1, device=device_local)\n\n        # initial tokens: cond token (based on t=1) and random feature tokens\n        cond_tok = self._cond_token(torch.full((B,), 1.0, device=device_local), cond_vec, do_cfg_dropout=False)  # [B, d]\n        feat_tok = torch.randn(B, self.num_numeric + self.num_cat, self.token_dim, device=device_local)\n\n        # helper to run denoiser and get eps_pred with CFG\n        def get_eps_pred(tokens_feat: torch.Tensor, t_val: torch.Tensor, cond_in: torch.Tensor, guidance_w_local: float):\n            \"\"\"\n            tokens_feat: [B, Tfeat, d] current feature tokens\n            returns eps_pred: [B, Tfeat, d]\n            uses classifier-free guidance:\n              eps = (1+w) * eps_cond - w * eps_uncond\n            \"\"\"\n            # build full tokens with cond token filled (cond or uncond)\n            # cond_cond: cond token for cond_in; cond_uncond: cond token with zeros\n            cond_c = self._cond_token(t_val, cond_in, do_cfg_dropout=False)      # [B, d]\n            cond_u = self._cond_token(t_val, torch.zeros_like(cond_in), do_cfg_dropout=False)  # [B,d]\n\n            tokens_c = torch.cat([cond_c.unsqueeze(1), tokens_feat], dim=1)\n            tokens_u = torch.cat([cond_u.unsqueeze(1), tokens_feat], dim=1)\n\n            # run denoiser both cond and uncond (note: deterministic, no grad at sample time)\n            den_c = self.denoiser(tokens_c)  # [B, 1+Tfeat, d]\n            den_u = self.denoiser(tokens_u)\n            eps_c = self.noise_head(den_c[:, 1:, :])\n            eps_u = self.noise_head(den_u[:, 1:, :])\n\n            eps = (1.0 + guidance_w_local) * eps_c - guidance_w_local * eps_u\n            return eps  # [B, Tfeat, d]\n\n        # sampling loop (DDIM-like deterministic updates)\n        for s in range(steps):\n            t_cur = t_grid[s].repeat(B)\n            t_next = t_grid[s + 1].repeat(B)\n            alpha_cur, sigma_cur = self._alpha_sigma(t_cur)   # [B, Tfeat, d]\n            alpha_next, sigma_next = self._alpha_sigma(t_next)\n\n            # compute eps via CFG\n            eps = get_eps_pred(feat_tok, t_cur, cond_vec, guidance_w)\n\n            # optional detector-guidance: compute gradient w.r.t feat_tok to adjust eps direction\n            if detector_fn is not None and det_guidance_w != 0.0:\n                # we need grad through detector_fn; enable grad for feat_tok temporarily\n                feat_tok.requires_grad_(True)\n                # decode to current categorical ids & numeric readout for detector\n                # decode numerics as simple readout via num_readout heads (mean of token dims or learned readout)\n                # We will produce a temporary numeric array and cat ids for the detector\n                # numeric readout\n                tok_num, tok_cat = feat_tok[:, :self.num_numeric, :], feat_tok[:, self.num_numeric:, :]\n                num_readout = []\n                for j, proj in enumerate(self.num_readout):\n                    num_readout.append(proj(tok_num[:, j, :]).squeeze(-1))\n                num_readout = torch.stack(num_readout, dim=1) if len(num_readout) > 0 else torch.zeros(B,0, device=device_local)\n                # categorical decode (argmax on logits from decoders)\n                cat_logits = [dec(tok_cat[:, idx, :]) for idx, dec in enumerate(self.cat_decoders)]\n                cat_ids = torch.stack([logits.argmax(dim=-1) for logits in cat_logits], dim=1) if len(cat_logits)>0 else torch.zeros(B,0, dtype=torch.long, device=device_local)\n\n                # compute detector objective (e.g., fraud logit); detector_fn must accept (num_readout, cat_ids, cond maybe)\n                # detector_fn should return [B] logits (higher -> more fraud)\n                detector_logits = detector_fn(num_readout, cat_ids)  # user-provided detector fn\n                # want gradient of detector_logits wrt feat_tok\n                grad = torch.autograd.grad(detector_logits.sum(), feat_tok, retain_graph=False, create_graph=False)[0]\n                # normalize gradient\n                grad_norm = grad.view(B, -1).norm(dim=1).view(B,1,1).clamp_min(1e-8)\n                grad_dir = grad / grad_norm\n                # modify eps to push generation in the direction that changes detector output:\n                # if det_guidance_w > 0, we want to *increase* detector_logits (make more fraudulent);\n                # if det_guidance_w < 0, we reduce detector_logits (make more benign)\n                eps = eps + det_guidance_w * grad_dir.detach()\n                feat_tok.requires_grad_(False)\n\n            # predict x0 (per token) and update feat_tok deterministically (DDIM-like)\n            # x0 = (feat_tok - sigma_cur * eps) / (alpha_cur + 1e-8)\n            x0_est = (feat_tok - sigma_cur * eps) / (alpha_cur + 1e-6)\n            feat_tok = alpha_next * x0_est + sigma_next * eps\n\n            # Mixed-type stochastic sampler mechanics:\n            # every decode_every steps, decode categorical tokens to discrete ids, re-embed them, and optionally \"refresh\"\n            if (s % decode_every) == 0 and self.num_cat > 0:\n                tok_num = feat_tok[:, :self.num_numeric, :]  # [B, Fnum, d]\n                tok_cat = feat_tok[:, self.num_numeric:, :]  # [B, C, d]\n                # decode categorical logits from decoders\n                cat_logits = [dec(tok_cat[:, idx, :]) for idx, dec in enumerate(self.cat_decoders)]\n                if stochastic_decode:\n                    # Gumbel sampling for stochastic decode\n                    cat_ids = []\n                    for logits in cat_logits:\n                        g = -torch.log(-torch.log(torch.rand_like(logits) + 1e-8) + 1e-8)\n                        cat_ids.append(torch.argmax(logits + g, dim=-1))\n                    cat_ids = torch.stack(cat_ids, dim=1)  # [B, C]\n                else:\n                    cat_ids = torch.stack([logits.argmax(dim=-1) for logits in cat_logits], dim=1)\n\n                # re-embed categorical ids to token space\n                re_emb_list = []\n                for j, emb in enumerate(self.tokenizer.cat_embs):\n                    re_emb_list.append(emb(cat_ids[:, j]))  # [B, token_dim]\n                re_emb = torch.stack(re_emb_list, dim=1)  # [B, C, token_dim]\n\n                # forward refresh: with prob p_refresh per (B, C), add small Gaussian noise to re-emb to avoid collapse\n                if p_refresh > 0:\n                    mask = (torch.rand(B, self.num_cat, 1, device=device_local) < p_refresh).float()\n                    noise = torch.randn_like(re_emb) * refresh_sigma\n                    re_emb = (1 - mask) * re_emb + mask * (re_emb + noise)\n\n                # stitch back numeric tokens unchanged + refreshed cat emb\n                feat_tok = torch.cat([tok_num, re_emb], dim=1)\n\n        # after loop: finalize numeric & categorical outputs\n        tok_num = feat_tok[:, :self.num_numeric, :] if self.num_numeric > 0 else torch.zeros(B,0,device=device_local)\n        tok_cat = feat_tok[:, self.num_numeric:, :] if self.num_cat > 0 else torch.zeros(B,0,device=device_local)\n\n        # numeric readout: apply readout heads to token vectors -> scalar features\n        if self.num_numeric > 0:\n            nums = []\n            for j, proj in enumerate(self.num_readout):\n                nums.append(proj(tok_num[:, j, :]).squeeze(-1))\n            x_num_out = torch.stack(nums, dim=1)  # [B, Fnum]\n        else:\n            x_num_out = torch.zeros(B,0, device=device_local)\n\n        # final categorical decode (deterministic argmax)\n        cat_ids_out = []\n        for j, dec in enumerate(self.cat_decoders):\n            logits = dec(tok_cat[:, j, :])\n            cat_ids_out.append(torch.argmax(logits, dim=-1))\n        if len(cat_ids_out) > 0:\n            x_cat_out = torch.stack(cat_ids_out, dim=1)  # [B, C]\n        else:\n            x_cat_out = torch.zeros(B,0, dtype=torch.long, device=device_local)\n\n        return x_num_out, x_cat_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # -------------------------\n# # Example training & sampling usage\n# # -------------------------\n# if __name__ == \"__main__\":\n#     # toy shapes - adapt to your dataset\n#     FNUM = 5\n#     CATS = [10, 8, 6]          # three categorical columns with cardinalities\n#     COND_DIM = 64\n#     BATCH = 16\n\n#     gen = TabDiffusionGenerator(num_numeric=FNUM, cat_cardinalities=CATS, cond_dim=COND_DIM,\n#                                 token_dim=192, time_embed_dim=64,\n#                                 transformer_layers=4, transformer_heads=4, transformer_ff=512,\n#                                 uncond_prob=0.1).to(device)\n\n#     # toy batch\n#     x_num = torch.randn(BATCH, FNUM, device=device)             # normalized numeric\n#     x_cat = torch.randint(0, 4, (BATCH, len(CATS)), device=device)\n#     cond = torch.randn(BATCH, COND_DIM, device=device)          # cond vec includes fraud label\n\n#     # training loss (single step)\n#     gen.train()\n#     loss, info = gen.training_loss(x_num, x_cat, cond)\n#     print(\"Training loss:\", loss.item())\n\n#     # sampling (both classes): when using a real dataset, provide cond vectors with fraud label 0 or 1 accordingly\n#     gen.eval()\n#     # example cond batches (non-fraud and fraud contexts)\n#     cond_nonfraud = torch.randn(1, COND_DIM, device=device)\n#     cond_fraud = torch.randn(1, COND_DIM, device=device)  # in practice set cond to appropriate account/device embedding + label\n#     x_num_samp, x_cat_samp = gen.sample(batch_size=8, cond_vec=cond_nonfraud, steps=50, guidance_w=1.5, stochastic_decode=True)\n#     print(\"Sample num shape:\", x_num_samp.shape, \"cat shape:\", x_cat_samp.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Test TabDiffusion with a real benchmark (IEEE-CIS)**","metadata":{}},{"cell_type":"code","source":"train_trans = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_id    = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\nprint(train_trans.shape, train_id.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# merge\ndf = train_trans.merge(train_id, on='TransactionID', how='left')\nprint(\"Merged shape:\", df.shape)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# drop identifier columns you won't model\ndrop_cols = ['TransactionID']  # keep TransactionDT for time_to_detect\ncandidate = df.drop(columns=drop_cols)\n\n# identify label\nlabel_col = 'isFraud'\n\n# numeric / categorical heuristics\nnum_cols = candidate.select_dtypes(include=['int64','float64']).columns.tolist()\nnum_cols = [c for c in num_cols if c != label_col]\ncat_cols = candidate.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numeric cols:\", len(num_cols))\nprint(\"Categorical cols:\", len(cat_cols))\n# Optionally prune columns to avoid very high-cardinality ones (or group rare categories)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill NAs\ndf[num_cols] = df[num_cols].fillna(0.0)\ndf[cat_cols] = df[cat_cols].fillna('NA')\n\n# scalers & encoders\nscaler = StandardScaler()\nscaler.fit(df[num_cols].values)   # save for inference\n\nlabel_encoders = {}\ncat_cardinalities = []\nfor c in cat_cols:\n    le = LabelEncoder()\n    df[c] = le.fit_transform(df[c].astype(str).values)\n    label_encoders[c] = le\n    cat_cardinalities.append(len(le.classes_))\n\nprint(\"cat_cardinalities:\", cat_cardinalities[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choose a handful of columns to form cond (identity/device + product)\ncond_columns = ['DeviceInfo', 'DeviceType', 'ProductCD']  # example: adjust if missing\ncond_columns = [c for c in cond_columns if c in df.columns]\n\n# We'll make small embedding layers per cond col and concatenate + the fraud label bit.\nCOND_EMBED_DIM = 32\n\n# Build embedding lookups (torch.nn.Embedding) and store mapping from column -> embedding index size\ncond_cardinalities = [int(df[c].nunique()) for c in cond_columns]\ncond_emb_modules = nn.ModuleDict({\n    c: nn.Embedding(n, COND_EMBED_DIM) for c, n in zip(cond_columns, cond_cardinalities)\n})\n# Move to device when used (cond builder below will create tensors on CPU/GPU accordingly)\n\ndef build_cond_vec_batch(df_rows):\n    \"\"\"\n    df_rows: pd.DataFrame (batch)\n    returns: torch.FloatTensor of shape [B, cond_dim]\n    cond_dim = len(cond_columns)*COND_EMBED_DIM + 1 (fraud bit)\n    \"\"\"\n    # collect embeddings then concat\n    parts = []\n    for c in cond_columns:\n        ids = torch.tensor(df_rows[c].values, dtype=torch.long, device=device)\n        parts.append(cond_emb_modules[c](ids))  # [B, COND_EMBED_DIM]\n    # fraud label as float (or leave as provided)\n    fraud_bit = torch.tensor(df_rows[label_col].values.reshape(-1,1), dtype=torch.float32, device=device)\n    if len(parts) > 0:\n        cond = torch.cat(parts + [fraud_bit], dim=1)  # [B, n*COND_EMBED_DIM + 1]\n    else:\n        cond = fraud_bit\n    return cond  # torch tensor\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class IEEECISTableDataset(Dataset):\n    def __init__(self, df, num_cols, cat_cols, label_col='isFraud', max_rows=None):\n        self.df = df if max_rows is None else df.sample(max_rows, random_state=42).reset_index(drop=True)\n        self.num_cols = num_cols\n        self.cat_cols = cat_cols\n        self.label_col = label_col\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        x_num = torch.tensor(scaler.transform(row[self.num_cols].values.reshape(1,-1)).squeeze(0), dtype=torch.float32)\n        x_cat = torch.tensor(row[self.cat_cols].values, dtype=torch.long)\n        cond_vec = build_cond_vec_batch(pd.DataFrame([row]))[0]  # cond builder returns [1, D]\n        y = torch.tensor(int(row[self.label_col]), dtype=torch.long)\n        tx_time = row.get('TransactionDT', np.nan)\n        # choose account id (card1) for time-to-detect grouping\n        acct = row.get('card1', None)\n        return x_num, x_cat, cond_vec, y, tx_time, acct\n\ndef collate_fn(batch):\n    x_num = torch.stack([b[0] for b in batch], dim=0)\n    x_cat = torch.stack([b[1] for b in batch], dim=0)\n    cond = torch.stack([b[2] for b in batch], dim=0)\n    y = torch.stack([b[3] for b in batch], dim=0)\n    tx_time = [b[4] for b in batch]\n    acct = [b[5] for b in batch]\n    return x_num, x_cat, cond, y, tx_time, acct\n\ndataset = IEEECISTableDataset(df, num_cols, cat_cols, label_col, max_rows=100000)  # start small\nloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shapes\nFNUM = len(num_cols)\nCATS = [int(df[c].nunique()) for c in cat_cols]\nCOND_DIM = len(cond_columns) * COND_EMBED_DIM + 1  # matches build_cond_vec\ngen = TabDiffusionGenerator(num_numeric=FNUM, cat_cardinalities=CATS, cond_dim=COND_DIM,\n                            token_dim=192, time_embed_dim=64, transformer_layers=4,\n                            transformer_heads=4, transformer_ff=512, uncond_prob=0.1).to(device)\n\nopt_g = torch.optim.AdamW(gen.parameters(), lr=2e-4, weight_decay=1e-4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen.train()\nEPOCHS = 6   # start small to validate pipeline\nfor epoch in range(EPOCHS):\n    running = 0.0\n    for i, (x_num, x_cat, cond, y, *_ ) in enumerate(loader):\n        x_num, x_cat, cond = x_num.to(device), x_cat.to(device), cond.to(device)\n        opt_g.zero_grad()\n        loss, info = gen.training_loss(x_num, x_cat, cond)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(gen.parameters(), 1.0)\n        opt_g.step()\n        running += loss.item()\n        if i % 50 == 0:\n            print(f\"Epoch {epoch} step {i} loss {running/(i+1):.4f}\")\n    # optionally save\n    torch.save(gen.state_dict(), f\"checkpoints/gen_epoch{epoch}.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build two representative cond vectors for non-fraud and fraud contexts:\n# Option A: take a real cond vector from dataset where isFraud==0 and one where isFraud==1\nsample_nonfraud_row = df[df[label_col]==0].iloc[:1]\nsample_fraud_row = df[df[label_col]==1].iloc[:1]\n\ncond_non = build_cond_vec_batch(sample_nonfraud_row).to(device)\ncond_fraud = build_cond_vec_batch(sample_fraud_row).to(device)\n\ngen.eval()\nwith torch.no_grad():\n    num_synth_non, cat_synth_non = gen.sample(batch_size=256, cond_vec=cond_non, steps=60, guidance_w=1.5)\n    num_synth_fraud, cat_synth_fraud = gen.sample(batch_size=256, cond_vec=cond_fraud, steps=60, guidance_w=1.5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def synth_to_df(x_num, x_cat, base_row):\n    # base_row: a real row we use to copy some static values like TransactionDT offsets, etc.\n    # x_num: [B, Fnum], x_cat: [B, C]\n    B = x_num.shape[0]\n    out = []\n    for i in range(B):\n        row = {}\n        for j, col in enumerate(num_cols):\n            row[col] = x_num[i, j].item()  # these are normalized; invert scaler if you want original scale\n        for j, col in enumerate(cat_cols):\n            row[col] = int(x_cat[i, j].item())\n        # copy some static columns from base_row to keep context (merchant, device text may be lost)\n        for col in ['TransactionDT','DeviceInfo','card1']:\n            if col in base_row:\n                row[col] = base_row[col].values[0]\n        out.append(row)\n    return pd.DataFrame(out)\n\ndf_synth_non = synth_to_df(num_synth_non.cpu(), cat_synth_non.cpu(), sample_nonfraud_row)\ndf_synth_non['isFraud'] = 0\ndf_synth_fraud = synth_to_df(num_synth_fraud.cpu(), cat_synth_fraud.cpu(), sample_fraud_row)\ndf_synth_fraud['isFraud'] = 1\n\n# join to real training subset to build detector training set\ntrain_df = pd.concat([df.sample(20000), df_synth_non.sample(2000), df_synth_fraud.sample(2000)], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import average_precision_score, roc_auc_score\n\n# Prepare features: numeric + label-encoded categoricals (or target encoder)\n# For XGBoost we convert cats to numeric indices (already done above)\nX = train_df[num_cols + cat_cols].fillna(0)\ny = train_df['isFraud'].astype(int)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndval = xgb.DMatrix(X_val, label=y_val)\n\nparams = {'objective':'binary:logistic', 'eval_metric':'auc', 'eta':0.1, 'max_depth':6}\nbst = xgb.train(params, dtrain, num_boost_round=200, evals=[(dval,'val')], early_stopping_rounds=20)\n\n# evaluate\ny_prob = bst.predict(dval)\nprint(\"AUPRC (val):\", average_precision_score(y_val, y_prob))\nprint(\"ROC AUC (val):\", roc_auc_score(y_val, y_prob))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score, precision_recall_curve, roc_auc_score\ndef auc_pr(y_true, y_score): return average_precision_score(y_true, y_score)\ndef precision_at_k(y_true, y_score, k):\n    arr = np.array(y_score); idx = np.argsort(-arr)[:k]\n    return np.mean(np.array(y_true)[idx])\n\ndef expected_calibration_error(y_true, y_prob, n_bins=10):\n    y_true, y_prob = np.array(y_true), np.array(y_prob)\n    bins = np.linspace(0,1,n_bins+1)\n    binids = np.digitize(y_prob, bins) - 1\n    ece = 0.0\n    for i in range(n_bins):\n        mask = binids == i\n        if mask.sum() == 0: continue\n        ece += (mask.sum()/len(y_prob)) * abs(y_true[mask].mean() - y_prob[mask].mean())\n    return ece\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def time_to_detect_per_account(df_eval, score_col='score', time_col='TransactionDT', label_col='isFraud', account_col='card1', threshold=0.5):\n    accounts = df_eval[account_col].unique()\n    delays = []\n    for acc in accounts:\n        acc_df = df_eval[df_eval[account_col]==acc].sort_values(time_col)\n        fraud_times = acc_df[acc_df[label_col]==1][time_col]\n        if len(fraud_times)==0: continue\n        first_fraud_time = fraud_times.iloc[0]\n        detected = acc_df[(acc_df[score_col] >= threshold)]\n        if detected.empty:\n            delays.append(np.nan)\n        else:\n            first_detect = detected[time_col].iloc[0]\n            delays.append(first_detect - first_fraud_time)\n    return np.nanmean(delays), delays\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}